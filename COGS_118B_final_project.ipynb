{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COGS 118B final project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faraznshaikh/Awesome-Design-Tools/blob/master/COGS_118B_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHwPpJFdYsWI",
        "colab_type": "text"
      },
      "source": [
        "Final project notebook:\n",
        "\n",
        "Ideas\n",
        "“We can make an algorithm that randomly samples from a bunch of datasets so our shit’s generalizable af” - Ribhu \n",
        "We can use tSNE to project n-facial features onto a 2-d graph and use some clustering algorithm to classify facial expressions\n",
        "\n",
        "Description: We are going to build a web application called Phace where the backend uses unsupervised learning to analyze and extrapolate information from facial features. The way we plan to implement this is to have a front end form which takes in a picture of human face which will be transported to our backend algorithm which will analyze it and give us information about certain characteristics of the face such as determining facial expressions such as smiles and frowns. We plan to implement the front end using React and Bootstrap and the backend using Node and Python.\n",
        "\n",
        "Pipeline 1:\n",
        "Image -> Dim Red (PCA or tSNE) -> Clustering -> Convolutional Neural Network (trained on cluster labels)\n",
        "\n",
        "Pipeline 2:\n",
        "Image -> Deep Autoencoder (Encoder CNN and Decoder CNN)\n",
        "\n",
        "Idea is to test out both the methods and compare them, then use the best method in the webapp.\n",
        "\n",
        "\n",
        "Datasets:\n",
        "\n",
        "*   US Faces: http://wilmabainbridge.com/facememorability2.html \n",
        "* http://www2.ece.ohio-state.edu/~aleix/ARdatabase.html\n",
        "\n",
        "\n",
        "\n",
        "Resources:\n",
        "* https://www.sciencedirect.com/science/article/abs/pii/S1005888513600382 - Hybrid PCA and LBP for extracting facial features\n",
        "* https://www.cl.cam.ac.uk/~pr10/publications/ssci15.pdf - Feature reduction\n",
        "* http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.9632&rep=rep1&type=pdf - Improving dim red\n",
        "* https://arxiv.org/ftp/arxiv/papers/1505/1505.04058.pdf - Facial expression classification in real time\n",
        "* https://www.ncbi.nlm.nih.gov/pubmed/27875237 - Autoencoders\n",
        "* https://github.com/ageitgey/face_recognition - Cluster code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtrB_QtVacMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import PIL \n",
        "import cv2\n",
        "import glob \n",
        "#import imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5QlvHeFdGCm",
        "colab_type": "code",
        "outputId": "bd68719a-acb8-4139-936a-a929b0ff65be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXcBPZDh7OhB",
        "colab_type": "code",
        "outputId": "3108c326-4916-4c5d-89d4-6674df7d9712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!ls /content/gdrive/Team\\ Drives/Cogs\\ 118B\\ Group\\ Project/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'118b Video.gslides'  'Link to Notebook.gdoc'   SadLandmarks\n",
            " cohn-kanade\t       misc\t\t        train\n",
            " colorferet.tar        MiscLandmarks\t       'trainNew (1)'\n",
            " dataset\t      'Project Planning.gdoc'   valid\n",
            " happy\t\t      'project talk.docx'       validNew\n",
            " HappyLandmarks       'project talk.gdoc'       vgg16_weights.h5\n",
            " images\t\t       sad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr0hL6ZdePwR",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr2vUA3ceWUj",
        "colab_type": "code",
        "outputId": "11830f66-a11e-43d6-caf7-f021fe22a9e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "from scipy import misc\n",
        "png_happy = []\n",
        "for image_path1 in glob.glob(\"/content/gdrive/Team Drives/Cogs 118B Group Project/happy/*.png\"):\n",
        "    png_happy.append(misc.imread(image_path1))  \n",
        "\n",
        "im_happy = np.asarray(png_happy)\n",
        "\n",
        "print ('Importing done...', im_happy.shape)\n",
        "print(im_happy[0])\n",
        "\n",
        "\n",
        "png_sad = []\n",
        "for image_path2 in glob.glob(\"/content/gdrive/Team Drives/Cogs 118B Group Project/sad/*.png\"):\n",
        "    png_sad.append(misc.imread(image_path2))  \n",
        "\n",
        "im_sad = np.asarray(png_sad)\n",
        "\n",
        "print ('Importing done...', im_sad.shape)\n",
        "print(im_sad[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importing done... (114, 490, 640)\n",
            "[[ 55 104 154 ...   0   0   0]\n",
            " [ 60 106 151 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " ...\n",
            " [ 39  42  42 ... 200 203 201]\n",
            " [ 40  41  40 ... 195 193 194]\n",
            " [ 39  42  42 ... 200 203 201]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importing done... (99, 490, 640)\n",
            "[[ 48 105 143 ...  78  43  21]\n",
            " [ 57 110 148 ...  67  36  16]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " ...\n",
            " [  9   8   7 ...  12  12  12]\n",
            " [  5   6  11 ...  13  13  11]\n",
            " [  9   8   7 ...  12  12  12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR60iKHaiGEE",
        "colab_type": "code",
        "outputId": "935dd6c2-eb68-474c-fc7b-8a01de384af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(im_sad[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 48 105 143 ...  78  43  21]\n",
            " [ 57 110 148 ...  67  36  16]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " ...\n",
            " [  9   8   7 ...  12  12  12]\n",
            " [  5   6  11 ...  13  13  11]\n",
            " [  9   8   7 ...  12  12  12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Z17HmzgGHp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NjJJNUKgGsp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKLnWvYseW-b",
        "colab_type": "text"
      },
      "source": [
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOsiGUWFeY5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from numpy import linalg as LA\n",
        "\n",
        "#perform PCA on each image in image_array and store in pca_array\n",
        "#pca = PCA(n_components=300)\n",
        "#pca_array = []\n",
        "#final_array = []\n",
        "\n",
        "#add all images together to find mean image\n",
        "for img_i in range(len(im_happy)):\n",
        "    if img_i == 0:\n",
        "      pca_array = im_happy[img_i]\n",
        "    else:\n",
        "      pca_array = np.add(pca_array, im_happy[img_i])\n",
        "    mean_array = np.array(pca_array)\n",
        "    mean_array = mean_array/len(im_happy)\n",
        "    \n",
        "#find eigvecs and eigvals\n",
        "Z = np.tile(mean_array,len(im_happy))\n",
        "vec,val = LA.eig(np.matmul(np.transpose(Z),Z))\n",
        "    \n",
        "#eigsort function for sorting in decreasing order\n",
        "def eigsort(V, D):\n",
        "  eigvals = np.diag(D)\n",
        "  lohival = np.sort(eigvals, axis=0)\n",
        "  lohiindex = np.argsort(eigvals, axis=0)\n",
        "  eigvec = np.flipud(lohival)\n",
        "  index = np.flipud(lohiindex)\n",
        "  Dsort = np.diag(eigvec)\n",
        "  M=len(eigvec)\n",
        "  Vsort = np.zeros((M,M))\n",
        "  for i in range(M):\n",
        "    Vsort[:,i] = V[:,index[i]]\n",
        "  return (Vsort, Dsort)\n",
        "\n",
        "#sort eigvecs and eigvals\n",
        "vec,val = eigsort(vec,val)\n",
        "\n",
        "#find principal components\n",
        "U = np.matmul(Z, vec)\n",
        "\n",
        "#normalize vectors in U\n",
        "for img_i in range(len(U)):\n",
        "  U[img_i] = LA.norm(U[img_i])\n",
        "\n",
        "#U now contains the principal components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuvkjdHLbG5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.misc\n",
        "n = 0\n",
        "for item1 in im_happy:\n",
        "  n= n +1 \n",
        "  outfile_happy = '%s.jpg' % (n)\n",
        "  scipy.misc.imsave(/outfile_happy,item2)\n",
        "  \n",
        "m = 0\n",
        "for item2 in im_sad:\n",
        "  m = m +1 \n",
        "  outfile_sad = '%s.jpg' % (m)\n",
        "  scipy.misc.imsave(outfile_sad,item)\n",
        "  \n",
        "  \n",
        "  \n",
        "#scipy.misc.imsave('outfile0.jpg',im[0])\n",
        "#scipy.misc.imsave('outfile1.jpg', pca_array[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDVpJAGddrUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "picture_arrayHappy =[]\n",
        "for pictures1 in glob.glob(\"*.jpg\"):\n",
        "  picture_arrayHappy.append(misc.imread(pictures1))\n",
        "  \n",
        "picture_arraySad =[]\n",
        "for pictures2 in glob.glob(\"*.jpg\"):\n",
        "  picture_arraySad.append(misc.imread(pictures2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHOIl7AcqK9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(picture_arraySad))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruzW-d3ReZSa",
        "colab_type": "text"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vozwRjt_eadZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO\n",
        "! pip install CMake\n",
        "! pip install Dlib\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9xkLnTNPrKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install face_recognition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yJUCmK8ssvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import face_recognition\n",
        "\n",
        "landmarks_array_Happy = []\n",
        "index_H = 0 \n",
        "for pictures in glob.glob(\"/content/gdrive/Team Drives/Cogs 118B Group Project/happy/*.png\"):\n",
        "      # Load the jpg file into a numpy array\n",
        "      image = face_recognition.load_image_file(pictures)\n",
        "\n",
        "      # Find all facial features in all the faces in the image\n",
        "      face_landmarks_list = face_recognition.face_landmarks(image)\n",
        "\n",
        "      #print(\"I found {} face(s) in this photograph.\".format(len(face_landmarks_list)))\n",
        "\n",
        "      # Create a PIL imagedraw object so we can draw on the picture\n",
        "      pil_image = Image.fromarray(image)\n",
        "      d = ImageDraw.Draw(pil_image)\n",
        "\n",
        "      for face_landmarks in face_landmarks_list:\n",
        "\n",
        "          # Print the location of each facial feature in this image\n",
        "          #for facial_feature in face_landmarks.keys():\n",
        "              #print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n",
        "\n",
        "          # Let's trace out each facial feature in the image with a line!\n",
        "          for facial_feature in face_landmarks.keys():\n",
        "              d.line(face_landmarks[facial_feature], width=5)\n",
        "\n",
        "      # Show the picture\n",
        "      pil_image.show()\n",
        "      \n",
        "      index_H = index_H+1\n",
        "      scipy.misc.imsave('/content/gdrive/Team Drives/Cogs 118B Group Project/HappyLandmarks/'+ '%s.png' %(index_H),pil_image)\n",
        "      \n",
        "      \n",
        "      landmarks_array_Happy.append(pil_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vipJ7mmV1FRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(landmarks_array_Happy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlJZRIxNtrLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "landmarks_array_Sad = []\n",
        "index_H = 0 \n",
        "for pictures in glob.glob(\"/content/gdrive/Team Drives/Cogs 118B Group Project/sad/*.png\"):\n",
        "      # Load the jpg file into a numpy array\n",
        "      image = face_recognition.load_image_file(pictures)\n",
        "\n",
        "      # Find all facial features in all the faces in the image\n",
        "      face_landmarks_list = face_recognition.face_landmarks(image)\n",
        "\n",
        "      #print(\"I found {} face(s) in this photograph.\".format(len(face_landmarks_list)))\n",
        "\n",
        "      # Create a PIL imagedraw object so we can draw on the picture\n",
        "      pil_image = Image.fromarray(image)\n",
        "      d = ImageDraw.Draw(pil_image)\n",
        "\n",
        "      for face_landmarks in face_landmarks_list:\n",
        "\n",
        "          # Print the location of each facial feature in this image\n",
        "          #for facial_feature in face_landmarks.keys():\n",
        "              #print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n",
        "\n",
        "          # Let's trace out each facial feature in the image with a line!\n",
        "          for facial_feature in face_landmarks.keys():\n",
        "              d.line(face_landmarks[facial_feature], width=5)\n",
        "\n",
        "      # Show the picture\n",
        "      pil_image.show()\n",
        "      \n",
        "      index_H = index_H+1\n",
        "      scipy.misc.imsave('/content/gdrive/Team Drives/Cogs 118B Group Project/SadLandmarks/'+ '%s.png' %(index_H),pil_image)\n",
        "      \n",
        "      \n",
        "      landmarks_array_Sad.append(pil_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec9ujwbeuxzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "landmarks_array_Misc = []\n",
        "index_H = 0 \n",
        "for pictures in glob.glob(\"/content/gdrive/Team Drives/Cogs 118B Group Project/misc/*.png\"):\n",
        "      # Load the jpg file into a numpy array\n",
        "      image = face_recognition.load_image_file(pictures)\n",
        "\n",
        "      # Find all facial features in all the faces in the image\n",
        "      face_landmarks_list = face_recognition.face_landmarks(image)\n",
        "\n",
        "      #print(\"I found {} face(s) in this photograph.\".format(len(face_landmarks_list)))\n",
        "\n",
        "      # Create a PIL imagedraw object so we can draw on the picture\n",
        "      pil_image = Image.fromarray(image)\n",
        "      d = ImageDraw.Draw(pil_image)\n",
        "\n",
        "      for face_landmarks in face_landmarks_list:\n",
        "\n",
        "          # Print the location of each facial feature in this image\n",
        "          #for facial_feature in face_landmarks.keys():\n",
        "              #print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n",
        "\n",
        "          # Let's trace out each facial feature in the image with a line!\n",
        "          for facial_feature in face_landmarks.keys():\n",
        "              d.line(face_landmarks[facial_feature], width=5)\n",
        "\n",
        "      # Show the picture\n",
        "      pil_image.show()\n",
        "      \n",
        "      index_H = index_H+1\n",
        "      scipy.misc.imsave('/content/gdrive/Team Drives/Cogs 118B Group Project/MiscLandmarks/'+ '%s.png' %(index_H),pil_image)\n",
        "      \n",
        "      \n",
        "      landmarks_array_Misc.append(pil_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-Z0af-cogcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scipy.misc.imsave('landmarksFace.jpg',pil_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DZbJwdNea4x",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-5JGPmpecqm",
        "colab_type": "code",
        "outputId": "07f5b94d-d030-4d4b-edaf-393f180d20a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1806
        }
      },
      "source": [
        "# CNN from scratch - Baseline model\n",
        "# Model from F_Chollet's script:\n",
        "# https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 640, 490\n",
        "\n",
        "train_data_dir = '/content/gdrive/Team Drives/Cogs 118B Group Project/trainNew'\n",
        "validation_data_dir = '/content/gdrive/Team Drives/Cogs 118B Group Project/validNew'\n",
        "nb_train_samples = 467\n",
        "nb_validation_samples = 114\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 3)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# this is the augmentation configuration we will use for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# this is the augmentation configuration we will use for testing:\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse')\n",
        "\n",
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=nb_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "validation_steps=nb_validation_samples // batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 462 images belonging to 3 classes.\n",
            "Found 114 images belonging to 3 classes.\n",
            "Epoch 1/50\n",
            "29/29 [==============================] - 302s 10s/step - loss: 0.3592 - acc: 0.6081 - val_loss: 0.2847 - val_acc: 0.6429\n",
            "Epoch 2/50\n",
            "29/29 [==============================] - 38s 1s/step - loss: 0.4266 - acc: 0.6272 - val_loss: 0.8134 - val_acc: 0.6429\n",
            "Epoch 3/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4313 - acc: 0.6275 - val_loss: 0.1627 - val_acc: 0.6633\n",
            "Epoch 4/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4313 - acc: 0.6269 - val_loss: 0.8134 - val_acc: 0.6020\n",
            "Epoch 5/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4171 - acc: 0.6278 - val_loss: 0.1627 - val_acc: 0.6020\n",
            "Epoch 6/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4219 - acc: 0.6281 - val_loss: 0.4880 - val_acc: 0.7041\n",
            "Epoch 7/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4029 - acc: 0.6281 - val_loss: 4.8657e-08 - val_acc: 0.6122\n",
            "Epoch 8/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4029 - acc: 0.6281 - val_loss: 0.6507 - val_acc: 0.6531\n",
            "Epoch 9/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4171 - acc: 0.6284 - val_loss: 0.4270 - val_acc: 0.6339\n",
            "Epoch 10/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4124 - acc: 0.6287 - val_loss: 0.4880 - val_acc: 0.6224\n",
            "Epoch 11/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4076 - acc: 0.6278 - val_loss: 0.9761 - val_acc: 0.6735\n",
            "Epoch 12/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4171 - acc: 0.6278 - val_loss: 0.1627 - val_acc: 0.6224\n",
            "Epoch 13/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4219 - acc: 0.6275 - val_loss: 0.8134 - val_acc: 0.6633\n",
            "Epoch 14/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4124 - acc: 0.6281 - val_loss: -0.3254 - val_acc: 0.6531\n",
            "Epoch 15/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4076 - acc: 0.6278 - val_loss: 0.1627 - val_acc: 0.6224\n",
            "Epoch 16/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4124 - acc: 0.6275 - val_loss: 0.6507 - val_acc: 0.6327\n",
            "Epoch 17/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4029 - acc: 0.6281 - val_loss: 0.5694 - val_acc: 0.6429\n",
            "Epoch 18/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4124 - acc: 0.6281 - val_loss: 0.4880 - val_acc: 0.6429\n",
            "Epoch 19/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.3886 - acc: 0.6272 - val_loss: 0.6507 - val_acc: 0.6122\n",
            "Epoch 20/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4124 - acc: 0.6275 - val_loss: -0.4880 - val_acc: 0.6020\n",
            "Epoch 21/50\n",
            "29/29 [==============================] - 40s 1s/step - loss: 0.3934 - acc: 0.6281 - val_loss: 0.8134 - val_acc: 0.7245\n",
            "Epoch 22/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4266 - acc: 0.6272 - val_loss: 0.9761 - val_acc: 0.5714\n",
            "Epoch 23/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4076 - acc: 0.6266 - val_loss: -0.3254 - val_acc: 0.6735\n",
            "Epoch 24/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4124 - acc: 0.6275 - val_loss: 0.6507 - val_acc: 0.6531\n",
            "Epoch 25/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4313 - acc: 0.6275 - val_loss: 0.1423 - val_acc: 0.6518\n",
            "Epoch 26/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4266 - acc: 0.6284 - val_loss: 0.4880 - val_acc: 0.6429\n",
            "Epoch 27/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4171 - acc: 0.6284 - val_loss: 0.1627 - val_acc: 0.6429\n",
            "Epoch 28/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4124 - acc: 0.6281 - val_loss: 0.8134 - val_acc: 0.6429\n",
            "Epoch 29/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4361 - acc: 0.6278 - val_loss: -0.8134 - val_acc: 0.6429\n",
            "Epoch 30/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4361 - acc: 0.6278 - val_loss: 1.7895 - val_acc: 0.6020\n",
            "Epoch 31/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4076 - acc: 0.6290 - val_loss: 0.9761 - val_acc: 0.6531\n",
            "Epoch 32/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4076 - acc: 0.6278 - val_loss: -0.1627 - val_acc: 0.6429\n",
            "Epoch 33/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4408 - acc: 0.6275 - val_loss: 0.2847 - val_acc: 0.6429\n",
            "Epoch 34/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4266 - acc: 0.6278 - val_loss: 0.6507 - val_acc: 0.6122\n",
            "Epoch 35/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.3886 - acc: 0.6266 - val_loss: -0.3254 - val_acc: 0.6531\n",
            "Epoch 36/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4076 - acc: 0.6266 - val_loss: 0.9761 - val_acc: 0.6327\n",
            "Epoch 37/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4219 - acc: 0.6275 - val_loss: 0.6507 - val_acc: 0.6531\n",
            "Epoch 38/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4171 - acc: 0.6278 - val_loss: 0.6507 - val_acc: 0.5918\n",
            "Epoch 39/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4029 - acc: 0.6281 - val_loss: -0.3254 - val_acc: 0.6939\n",
            "Epoch 40/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4124 - acc: 0.6287 - val_loss: 0.8134 - val_acc: 0.6429\n",
            "Epoch 41/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4266 - acc: 0.6266 - val_loss: 0.2847 - val_acc: 0.6429\n",
            "Epoch 42/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4076 - acc: 0.6278 - val_loss: 0.6507 - val_acc: 0.6327\n",
            "Epoch 43/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4029 - acc: 0.6287 - val_loss: 1.2164e-08 - val_acc: 0.6531\n",
            "Epoch 44/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4171 - acc: 0.6284 - val_loss: 1.1387 - val_acc: 0.6429\n",
            "Epoch 45/50\n",
            "29/29 [==============================] - 40s 1s/step - loss: 0.4266 - acc: 0.6272 - val_loss: 0.6507 - val_acc: 0.6327\n",
            "Epoch 46/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4171 - acc: 0.6272 - val_loss: -0.4880 - val_acc: 0.6429\n",
            "Epoch 47/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4219 - acc: 0.6269 - val_loss: 0.8134 - val_acc: 0.6224\n",
            "Epoch 48/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4076 - acc: 0.6278 - val_loss: 0.3254 - val_acc: 0.6531\n",
            "Epoch 49/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4124 - acc: 0.6275 - val_loss: 0.4270 - val_acc: 0.6339\n",
            "Epoch 50/50\n",
            "29/29 [==============================] - 39s 1s/step - loss: 0.4171 - acc: 0.6278 - val_loss: 0.8134 - val_acc: 0.6224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5c762135f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UHMDJ-UZa3c",
        "colab_type": "code",
        "outputId": "77b6128a-4594-40f3-d444-0a60095edd2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1878
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 640, 490\n",
        "\n",
        "train_data_dir = '/content/gdrive/Team Drives/Cogs 118B Group Project/trainNew'\n",
        "validation_data_dir = '/content/gdrive/Team Drives/Cogs 118B Group Project/validNew'\n",
        "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
        "nb_train_samples = 467\n",
        "nb_validation_samples = 114\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "\n",
        "bft = []\n",
        "bfv = []\n",
        "\n",
        "def save_bottlebeck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    # build the VGG16 network\n",
        "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    bottleneck_features_train = model.predict_generator(\n",
        "        generator, nb_train_samples // batch_size)\n",
        "    np.save(open('bottleneck_features_train.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, nb_validation_samples // batch_size)\n",
        "    np.save(open('bottleneck_features_validation.npy', 'wb'),\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
        "    train_labels = np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "    train_labels = train_labels[:len(train_labels)-2]\n",
        "\n",
        "    validation_data = np.load(open('bottleneck_features_validation.npy','rb'))\n",
        "    print(len(validation_data))\n",
        "    validation_labels = np.array([0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "    validation_labels = validation_labels[:len(validation_labels)-2]\n",
        "    print(len(validation_labels))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    result = model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights(top_model_weights_path)\n",
        "\n",
        "\n",
        "save_bottlebeck_features()\n",
        "train_top_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 466 images belonging to 3 classes.\n",
            "Found 114 images belonging to 3 classes.\n",
            "112\n",
            "112\n",
            "Train on 464 samples, validate on 112 samples\n",
            "Epoch 1/50\n",
            "464/464 [==============================] - 2s 5ms/step - loss: 7.8179 - acc: 0.4957 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 2/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 3/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 4/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 5/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 6/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 7/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 8/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 9/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 10/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 11/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 12/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 13/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 14/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 15/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 16/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 17/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 18/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 19/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 20/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 21/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 22/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 23/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 24/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 25/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 26/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 27/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 28/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 29/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 30/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 31/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 32/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 33/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 34/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 35/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 36/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 37/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 38/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 39/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 40/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 41/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 42/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 43/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 44/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 45/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 46/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 47/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 48/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 49/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "Epoch 50/50\n",
            "464/464 [==============================] - 1s 3ms/step - loss: 8.0243 - acc: 0.5022 - val_loss: 7.9151 - val_acc: 0.5089\n",
            "[7.817869169958707, 8.024310506623367, 8.024310506623367, 8.024310490180707, 8.024310473738046, 8.024310523066028, 8.024310523066028, 8.024310490180707, 8.024310457295385, 8.024310506623367, 8.024310457295385, 8.024310506623367, 8.024310490180707, 8.024310407967404, 8.024310539508688, 8.024310539508688, 8.024310506623367, 8.024310457295385, 8.024310440852725, 8.024310506623367, 8.024310523066028, 8.024310506623367, 8.024310440852725, 8.024310424410064, 8.024310539508688, 8.024310440852725, 8.024310523066028, 8.024310407967404, 8.024310457295385, 8.024310523066028, 8.024310523066028, 8.024310490180707, 8.024310473738046, 8.024310506623367, 8.024310473738046, 8.024310490180707, 8.024310523066028, 8.024310539508688, 8.024310440852725, 8.024310506623367, 8.024310473738046, 8.024310490180707, 8.024310490180707, 8.024310523066028, 8.024310490180707, 8.024310490180707, 8.024310523066028, 8.024310523066028, 8.024310407967404, 8.024310473738046]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4brnSEnakxt",
        "colab_type": "code",
        "outputId": "72baa8c2-f6a0-4ca8-ea5c-89868d1db528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1904
        }
      },
      "source": [
        "# Using pre-trained VGG16 weights\n",
        "# Model from F_Chollet's script:\n",
        "# https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975\n",
        "\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "# path to the model weights files.\n",
        "weights_path = '/content/gdrive/Team Drives/Cogs 118B Group Project/vgg16_weights.h5'\n",
        "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 640, 490\n",
        "\n",
        "train_data_dir = '/content/gdrive/Team Drives/Cogs 118B Group Project/trainNew'\n",
        "validation_data_dir = '/content/gdrive/Team Drives/Cogs 118B Group Project/validNew'\n",
        "nb_train_samples = 467\n",
        "nb_validation_samples = 114\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "\n",
        "# build the VGG16 network\n",
        "model = applications.VGG16(weights='imagenet', include_top=False, input_shape = (img_height,img_width,3))\n",
        "print('Model loaded.')\n",
        "\n",
        "fresh_m = Sequential()\n",
        "for layer in model.layers:\n",
        "  fresh_m.add(layer)\n",
        "  \n",
        "# build a classifier model to put on top of the convolutional model\n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
        "top_model.add(Dense(256, activation='relu'))\n",
        "top_model.add(Dropout(0.5))\n",
        "top_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# note that it is necessary to start with a fully-trained\n",
        "# classifier, including the top classifier,\n",
        "# in order to successfully do fine-tuning\n",
        "top_model.load_weights(top_model_weights_path)\n",
        "\n",
        "# add the model on top of the convolutional base\n",
        "# model = Model(inputs= model.input, outputs= top_model(model.output))\n",
        "fresh_m.add(top_model)\n",
        "\n",
        "# set the first 25 layers (up to the last conv block)\n",
        "# to non-trainable (weights will not be updated)\n",
        "for layer in model.layers[:25]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile the model with a SGD/momentum optimizer\n",
        "# and a very slow learning rate.\n",
        "fresh_m.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# prepare data augmentation configuration\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "# fine-tune the model\n",
        "result = fresh_m.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=nb_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "nb_val_samples=nb_validation_samples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n",
            "Found 466 images belonging to 3 classes.\n",
            "Found 114 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=29, epochs=50, validation_data=<keras_pre..., validation_steps=114)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "29/29 [==============================] - 144s 5s/step - loss: 15.7360 - acc: 0.1961 - val_loss: 15.6825 - val_acc: 0.1935\n",
            "Epoch 2/50\n",
            "29/29 [==============================] - 128s 4s/step - loss: 15.4310 - acc: 0.2088 - val_loss: 15.7419 - val_acc: 0.1916\n",
            "Epoch 3/50\n",
            "29/29 [==============================] - 128s 4s/step - loss: 15.2571 - acc: 0.2152 - val_loss: 15.6726 - val_acc: 0.1935\n",
            "Epoch 4/50\n",
            "29/29 [==============================] - 127s 4s/step - loss: 15.8051 - acc: 0.1899 - val_loss: 15.6787 - val_acc: 0.1933\n",
            "Epoch 5/50\n",
            "29/29 [==============================] - 128s 4s/step - loss: 16.1097 - acc: 0.1855 - val_loss: 15.6924 - val_acc: 0.1923\n",
            "Epoch 6/50\n",
            "29/29 [==============================] - 128s 4s/step - loss: 15.5269 - acc: 0.2006 - val_loss: 15.7320 - val_acc: 0.1904\n",
            "Epoch 7/50\n",
            "29/29 [==============================] - 128s 4s/step - loss: 15.8747 - acc: 0.1791 - val_loss: 15.6627 - val_acc: 0.1966\n",
            "Epoch 8/50\n",
            "29/29 [==============================] - 127s 4s/step - loss: 15.5353 - acc: 0.2001 - val_loss: 15.6887 - val_acc: 0.1927\n",
            "Epoch 9/50\n",
            "29/29 [==============================] - 127s 4s/step - loss: 15.4310 - acc: 0.2131 - val_loss: 15.7023 - val_acc: 0.1923\n",
            "Epoch 10/50\n",
            "29/29 [==============================] - 127s 4s/step - loss: 15.3962 - acc: 0.2152 - val_loss: 15.6726 - val_acc: 0.1941\n",
            "Epoch 11/50\n",
            "28/29 [===========================>..] - ETA: 1s - loss: 15.7223 - acc: 0.1875"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-cd0256266d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m nb_val_samples=nb_validation_samples)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    232\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             workers=0)\n\u001b[0m\u001b[1;32m    235\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    344\u001b[0m                                  \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                                  str(generator_output))\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz1OV_HfehQi",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrmy-hOneiey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss = result.history['loss']\n",
        "val_loss   = result.history['val_loss']\n",
        "train_acc  = result.history['acc']\n",
        "val_acc    = result.history['val_acc']\n",
        "xc         = range(epochs)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(xc, train_loss)\n",
        "plt.plot(xc, val_loss)\n",
        "plt.legend(('training','validation'))\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2OYIuc7VS_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(xc, train_acc)\n",
        "plt.plot(xc, val_acc)\n",
        "plt.legend(('training','validation'))\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}